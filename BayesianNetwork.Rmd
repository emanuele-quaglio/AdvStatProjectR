---
title: "R Notebook"
output: html_notebook
---

```{r}
#install.packages("tidyverse")
#install.packages("conflicted")
#install.packages("ggraph")
#install.packages("foreach")
#install.packages("doParallel")
#install.packages("bnlearn")
#install.packages("actuar")
library(tidyverse)
library(igraph)
library(ggraph)
library(foreach)
library(doParallel)
library(conflicted)
library(bnlearn)
library(actuar, include.only = c("rztbinom"))
#library(dplyr, include.only = c("select", "mutate")
conflicts_prefer(dplyr::filter)
```

```{r}
rds_name<-function(title){
  return(paste0(title,as.numeric(Sys.time()) * 1000, '.rds'))
}
print_grouped_df <- function(grouped_df) {
  # Loop over each group
  groups <- group_split(grouped_df)
  group_vars <- group_vars(grouped_df)
  
  for (i in seq_along(groups)) {
    # Get the current group data
    current_group <- groups[[i]]
    
    # Extract the values of the grouping variables for this group
    group_keys <- current_group[1, group_vars, drop = FALSE]
    
    # Print the group header with key values
    cat("\n==== Group:", paste(names(group_keys), "=", as.character(group_keys), collapse = ", "), "====\n")
    
    # Print the current group
    print(current_group)
  }
}

create_bn_dag <- function(dag) {

    net_nodes <- names(dag)

    our.net.dag <- empty.graph(net_nodes)

    net_parents <- dag
   
    for (node in net_nodes) {
        if (all(net_parents[node] != "character(0)")) {
        for (single_parent in net_parents[[node]]) {
            our.net.dag <- set.arc(our.net.dag, from = single_parent, to = node)
                }
            }
        }
    return(our.net.dag)
}

options(paged.print = FALSE)

logfactorial<-function(n){
  if(n>=2){
    res<-sum(log(seq(2, n)))
  }else{
    res<-0
  }
  return(res)
}

# Helper Function: Prod_j
Prod_j <- function(df, i, parents, r) {
  #print(paste("Running Prod_j with i =", i))
  #print(paste("Parents:", paste(parents, collapse = ", ")))
  
  if (!is.null(parents) & length(parents) != 0) {
    #print("Grouping by parents...")
    ret <- df %>%
      group_by(across(all_of(parents))) %>%
      summarise(N_ij = n(), .groups = "keep") %>% 
      mutate(ratio= logfactorial(r[i] - 1)- logfactorial(N_ij + r[i] - 1))
  } else {
    #print("No parents to group by...")
    ret <- df %>%
      summarise(N_ij = n(), .groups = "keep") %>% 
      mutate(ratio= logfactorial(r[i] - 1) - logfactorial(N_ij + r[i] - 1))
  }
  
  #print("Intermediate result of Prod_j:")
  #print(ret)
  
  result <- sum(ret$ratio)
  #print(paste("Result of Prod_j:", result))
  return(result)
}

# Helper Function: Prod_jk
Prod_jk <- function(df, i, parents) {
  #print(paste("Running Prod_jk with i =", i))
  #print(paste("Parents:", paste(parents, collapse = ", ")))
  
  if (length(c(parents, i)) != 0) {
    #print("Grouping by parents and i...")
    ret <- df %>%
      group_by(across(all_of(c(parents, i)))) %>%
      summarise(count = n(), .groups = "keep") %>%
      mutate(factorial_count = logfactorial(count))
    
    #print("Intermediate result of Prod_jk:")
    #print(ret)
    
    result <- sum(ret$factorial_count)
    #print(paste("Result of Prod_jk:", result))
    return(result)
  } else {
    #print("No grouping needed for Prod_jk.")
    return(1)
  }
}

# Main Function: f_20
f_20 <- function(original_df, i, parents, r) {
  #print(paste("Running f_20 with i =", i))
  #print(paste("Parents:", paste(parents, collapse = ", ")))
  
  prod_g_ij <- Prod_j(original_df, i, parents, r)
  prod_f_ijk <- Prod_jk(original_df, i, parents)
  
  #print(paste("prod_g_ij:", prod_g_ij))
  #print(paste("prod_f_ijk:", prod_f_ijk))
  
  result <- prod_g_ij + prod_f_ijk 
  #print(paste("Result of f_20:", result))
  
  return(result)
}

# Algorithm
k2_algorithm <- function(data, nodes=NULL, max_parents = NULL) {
  
  if(is.null(nodes)){
    nodes<-names(data)
  }
  if(is.null(max_parents)){
    max_parents<-length(nodes)-1
  }
  # Initialize the network
  network <- setNames(vector("list", length(nodes)), nodes)
  network[[nodes[1]]]<-character(0)
  #network[] <- lapply(network, function(x) character(0))
  #E' PROBABILE CHE CI VOGLIA QUALCOSA DI PIU'SPECIFICO DI UNA LIST, ES. net.dag <- empty.graph(nodes=nodes) 
  
  # Detect the number of available cores
  numCores <- parallel::detectCores()
  
  # Create a cluster with the desired number of cores
  cl <- makeCluster(numCores - 1)  # Use one less core than available
  
  # Register the cluster for parallel processing
  registerDoParallel(cl)
  # Ensure cluster is stopped even if an error occurs
  on.exit(stopCluster(cl))
  # Parallel execution example
  #stopCluster(cl)
  
  #Calculate r_i
  r<-map_int(data, n_distinct)
  # Iterate over each node
  result<-foreach (i = 2:length(nodes), .combine = 'c', .packages = c('dplyr', 'purrr'), .export = c('logfactorial', 'Prod_jk', 'Prod_j', 'f_20')) %dopar% {
     # Calculate r_i:

    parents <- c()
    score_old <- f_20(data, i, parents, r)
    proceed <- TRUE
    while (proceed & length(parents) < max_parents) {

      # Find the best candidate parent
      best_parent <- NULL
      best_score <- score_old

      for (z in setdiff(1:(i-1), parents)) {
        parents_trial<-c(parents, z)
        best_score <- f_20(data, i, parents_trial, r)
        if (best_score > score_old) { 
          best_parent <- z
          score_old <- best_score
        }
      }

      # Update parents and score if necessary
      if (!is.null(best_parent)) {
        parents <- c(parents, best_parent)
        score_old <- best_score
      } else {
        proceed <- FALSE
      }
    }
    
    # Return the parents for the current node
    return(list(list(node = nodes[i], foundparents = if(length(parents) == 0) character(0) else sapply(parents, function(el) nodes[el]), best_score=best_score)))

  }
  #print(result)
  # Assign results to the network
  total_best_score<-0
  for (res in result) {
    #print('_____________________')
    #print(typeof(res))
    #print(str(res))
    #print('res__________')
    #print(res)
    #print('resnode__________')
    #print(res$node)
    #print('resparents__________')
    #print(res$foundparents)
    network[[res$node]] <- res$foundparents
    total_best_score<-total_best_score+res$best_score
  }
  
  return(list(dag=network, score=total_best_score))
}

```

```{r}
print_network<-function(network, layout = "auto"){ 
  # Crea un dataframe con tutti i nodi 
  nodes <- tibble(node = names(network)) 
   
  # Crea un dataframe con gli archi esistenti (quelli t.c. nodi_non_isolati <- V(graph)[degree(graph) > 0]) 
  edges_existing <- tibble( 
    from = unlist(network), 
    to = rep(names(network), sapply(network, length)) 
  ) 
   
  # Identifica i nodi isolati 
  nodes_isolated <- nodes %>% 
    filter(!(node %in% c(edges_existing$from, edges_existing$to))) 
   
  # Crea un dataframe con gli archi autoreferenziali per i nodi isolati 
  edges_isolated <- tibble( 
    from = nodes_isolated$node, 
    to = nodes_isolated$node 
  ) 
     
# Combina i due dataframe degli archi 
edges <- bind_rows(edges_existing, edges_isolated) 
 
  # Creazione del grafo 
  graph <- graph_from_data_frame(edges, directed = TRUE) 
  ggraph(graph, layout = layout) + 
    geom_node_point(shape = 21, size = 20, color = "green") + 
    geom_node_text(aes(label = name), repel = FALSE, fontface = "bold", nudge_x = +0, nudge_y = +0) + 
    geom_edge_link(arrow = arrow(length = unit(0.2, "inches"), type = "closed"), 
                   end_cap = circle(), start_cap = circle()) + 
                   #, curve = 0.2) + 
    labs(title = "Rete Bayesiana") 
}



```

```{r}
trial_df <- data.frame(
  x1 = c(1, 1, 0, 1, 0, 0, 1, 0, 1, 0),
  x2 = c(0, 1, 0, 1, 0, 1, 1, 0, 1, 0),
  x3 = c(0, 1, 1, 1, 0, 1, 1, 0, 1, 0)
)

trial_df_zeros <- data.frame(
  x1 = c(0, 1, 0, 1, 0, 0, 0, 0, 1, 0),
  x2 = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
  x3 = c(0, 1, 0, 1, 0, 1, 1, 0, 1, 0),
  x4 = c(0, 1, 1, 1, 0, 1, 1, 0, 1, 0)
)

```

```{r}
network<-k2_algorithm(trial_df)
print(network[['dag']])
print(network[['score']])
print_network(network[['dag']])
```

```{r}
asia <- read_csv('https://www.ccd.pitt.edu/wiki/images/ASIA10k.csv')
head(asia)
asia <- asia |> select(asia, smoke, tub, lung, either, bronc, xray, dysp)
true_dag.asia<- model2network("[asia][smoke][tub|asia][lung|smoke][bronc|smoke][dysp|bronc:either][either|tub:lung][xray|either]")
```

```{r}
#asia_net <- k2.iter(dataset=asia, parents.nmax=2, f=log.f, k2=k2, n.iter=5, seed=1, type='bde', iss=10)

network<-k2_algorithm(asia)
#print(network)
print_network(network[['dag']])
```

```{r}
child <- read_csv('https://www.ccd.pitt.edu/wiki/images/CHILD10k.csv')
child <- child |> select(BirthAsphyxia, Disease, Sick, DuctFlow, CardiacMixing, LungParench, LungFlow, LVH, Age, Grunting, HypDistrib, HypoxiaInO2, CO2, ChestXray, LVHreport, GruntingReport, LowerBodyO2, RUQO2, CO2Report, XrayReport)
head(child)
true_dag.child<-model2network("[BirthAsphyxia][Disease|BirthAsphyxia][Sick|Disease][DuctFlow|Disease][CardiacMixing|Disease][LungParench|Disease][LungFlow|Disease][LVH|Disease][Age|Disease:Sick][Grunting|Sick:LungParench][HypDistrib|DuctFlow:CardiacMixing][HypoxiaInO2|CardiacMixing:LungParench][CO2|LungParench][ChestXray|LungParench:LungFlow][LVHreport|LVH][GruntingReport|Grunting][LowerBodyO2|HypDistrib:HypoxiaInO2][RUQO2|HypoxiaInO2][CO2Report|CO2][XrayReport|ChestXray]")
```

```{r}
network<-k2_algorithm(child)
#print(network)
print_network(network[['dag']])
```

```{r}
sachs <- read_csv('https://www.ccd.pitt.edu/wiki/images/SACHS10k.csv')
sachs <- sachs |> select(PKC, Plcg, PKA, PIP3, Raf, Jnk, P38, PIP2, Mek, Erk, Akt)
head(sachs)
true_dag.sachs <- model2network("[PKC][PKA|PKC][Raf|PKC:PKA][Mek|PKC:PKA:Raf][Erk|Mek:PKA][Akt|Erk:PKA][P38|PKC:PKA][Jnk|PKC:PKA][Plcg][PIP3|Plcg][PIP2|Plcg:PIP3]")
```

```{r}
network<-k2_algorithm(sachs)
#print(network)
print_network(network[['dag']])
```

```{r}
datasets<-list(asia=asia, child=child, sachs=sachs)
true_dags<-list(asia=true_dag.asia, child=true_dag.child, sachs=true_dag.sachs)
true_max_parents<-list(asia=2, child=2, sachs=3)
```

Comparison between predicted networks and ground truth (expected) networks, as a function of dataset size.
```{r}
f1_error<-function(my_dag, true_dag, n_true_edges=NULL){
  if(is.null(n_true_edges)){
    n_true_edges<-nrow(arcs(true_dag))
  }
  dag_compare <- bnlearn::compare(true_dag, my_dag) 
  precision<-as.numeric(dag_compare["tp"])/(as.numeric(dag_compare["tp"])+as.numeric(dag_compare["fp"])) 
  recall<-as.numeric(dag_compare["tp"])/n_true_edges  
  f1<-2*precision*recall/(precision+recall) 
  return(1-f1) 
}
```

```{r}
error_vs_data_size<-function(network_learner, data_df, true_dag){
  spanned=c(10,20,30,50,100,200,300, 500, 1000, 5000, 10000)
  errors=vector('list', length(spanned))
  for(i in 1:length(spanned)){
    my_dag<-create_bn_dag(network_learner(head(data_df, spanned[i]))[['dag']])
    errors[[i]]<-f1_error(my_dag, true_dag)
  }
  plot(spanned[1:length(errors)],errors,xlab='Dataset size',ylab='Error',type='l',col='blue') 
  return(list(x=spanned[1:length(errors)], y=errors))
}
```
First case: initialization with expected nodes ordering and expected max number of parents.
```{r}
constrained_nodes_parents_error_vs_datasize<-setNames(vector("list", 3), c("asia", "child", "sachs"))
CNPEVD<-constrained_nodes_parents_error_vs_datasize
for( dataset_name in c('asia','child','sachs')){
  CNPEVD[[dataset_name]]<-error_vs_data_size(function(data)(k2_algorithm(data, max_parents = 2)), datasets[[dataset_name]], true_dags[[dataset_name]])
}
saveRDS(CNPEVD, file=rds_name('CNPEVD'))
```

Second case: initialization with expected nodes ordering but 'unconstrained' number of parents.
```{r}
constrained_nodes_error_vs_datasize<-error_vs_data_size(k2_algorithm, child, true_dag.child)
constrained_nodes_error_vs_datasize<-setNames(vector("list", 3), c("asia", "child", "sachs"))
CNEVD<-constrained_nodes_error_vs_datasize
for( dataset_name in c('asia','child','sachs')){
  CNPEVD[[dataset_name]]<-error_vs_data_size(k2_algorithm, datasets[[dataset_name]], true_dags[[dataset_name]])
}
```


##Function that finds best nodes ordering with mcmc-metropolis-hastings-like procedure.

```{r}
uniform_transposition_generator<-function(nodes){
  # Choose two distinct indices randomly
  indices <- sample(seq_along(nodes), 2)
  
  # Swap the elements at these indices
  transposed_nodes <- nodes
  temp <- transposed_nodes[indices[1]]
  transposed_nodes[indices[1]] <- transposed_nodes[indices[2]]
  transposed_nodes[indices[2]] <- temp
  return(transposed_nodes)
}
ztbinom_permutation_generator<-function(nodes,  mean=1,variance=4/5){ 
 
  p<-(1-(variance/mean)) 
  n<-mean/p 
  n_transpositions<-rztbinom(1,size=n,prob=p) 
  #x<-seq(1,n,by=1) 
  #plot(x,dztbinom(x,size=n,prob=p)) 
  #perform the transposition 
  temp_nodes<-nodes     
  for(j in 1:n_transpositions){ 
    temp_nodes<-uniform_transposition_generator(temp_nodes)   
  } 
  transposed_nodes<-temp_nodes 
  return(transposed_nodes)   
}
bn_mcmc<-function(proposal_generator, score_function, data_df, n_iter){ 
  #proposal generator is a function that given an ordering, randomly samples a new ordering according to some distribution on the permutation space
  #score function is a function like k2_algorithm, that given an ordering and data, gives as output the list(network,score) of the corresponding best bayesian network
  nodes<-names(data_df)
  chain<-vector('list', n_iter)
  best_network<-score_function(data_df)
  best_score<-best_network[['score']]
  chain[[1]]<-list(nodes=nodes, dag=best_network[['dag']], score=best_score)
  old_best_score<-best_score
  
  for(i in 2:n_iter){
    
    proposal_nodes<-proposal_generator(nodes)
    proposal_data_df <- data_df |> select(all_of(proposal_nodes))
    proposal_best_network<-score_function(proposal_data_df)
    proposal_best_score<-proposal_best_network[['score']]
    if(runif(1) < min(1, exp(best_score-old_best_score))){ #we take exponential because we are dealing with log of probabilities
      nodes<-proposal_nodes
      #data_df <-proposal_data_df
      best_network<-proposal_best_network
    }
    chain[[i]]<-list(nodes=nodes, dag=best_network[['dag']], score=best_network[['score']])
  }
  max_index<-which.max(lapply(chain, function(el)(el[['score']])))
  return(list(chain=chain, best=chain[[max_index]]))
}
```

```{r}
n_iter<-5
start.time<-proc.time()
bn_mcmc_result<-bn_mcmc(ztbinom_permutation_generator, k2_algorithm, sachs, n_iter)
end.time<-proc.time()
cat('bn_mcmc duration',n_iter, 'iterations: ', end.time-start.time)
```

```{r}
saveRDS(bn_mcmc_result, file=rds_name('mcmc_ztbin'))
object.size(bn_mcmc_result[['chain']])
network_dag<-bn_mcmc_result[['best']][['dag']]
chain_scores<-sapply(bn_mcmc_result[['chain']], function(el)(el[['score']]))
chain_nodes_orderings<-lapply(bn_mcmc_result[['chain']], function(el)(el[['nodes']]))
print(bn_mcmc_result[['best']][['score']])
print_network(network_dag)
```

```{r}
print_scores_vs_ordering<-function(chain_scores, chain_nodes_orderings){
  # Sample data
  y <- chain_scores
  x <- chain_nodes_orderings
  #print(x)
  
  # Convert character vectors to single strings
  labels <- sapply(x, function(vec) paste(vec, collapse = " "))
  
  # Create a basic plot
  plot(y, xaxt = "n", xlab = "X-axis", ylab = "Y-axis", pch = 19, xlim = c(0,length(x)), ylim = c(min(chain_scores), max(chain_scores)))
  
  # Add custom x-axis labels
  axis(1, at = seq_along(y), labels = labels, las = 2)  # las = 2 rotates labels vertically
}
```

```{r}
max_score_vs_iter<-function(chain_scores){ 
  plot(1:length(chain_scores),cummax(chain_scores),xlab='Iteration',ylab='Running max score',col='blue',type='l') 
}
```

```{r}
print_scores_vs_ordering(chain_scores, chain_nodes_orderings)
```

```{r}
max_score_vs_iter(chain_scores)
```



##Function that finds best nodes ordering by uniformly sampling the corresponding permutation space.

```{r}
bn_uniform<-function(score_function, data_df, n_iter){
  #score function is a function like k2_algorithm, that given an ordering and data, gives as output the list(network,score) of the corresponding best bayesian network
  nodes<-names(data_df)
  chain<-vector('list', n_iter)
  for(i in 1:n_iter){
    best_network<-score_function(data_df)
    chain[[i]]<-list(nodes=nodes, dag=best_network[['dag']], score=best_network[['score']])
    nodes<-sample(nodes)
    data_df <- data_df |> select(all_of(nodes))
  }
  max_index<-which.max(lapply(chain, function(el)(el[['score']])))
  return(list(chain=chain, best=chain[[max_index]]))
}
```

```{r}
n_iter<-5
start.time<-proc.time()
bn_uniform_result<-bn_uniform(k2_algorithm, sachs, n_iter)
end.time<-proc.time()
cat('bn_ordering_sampling duration' ,n_iter, 'iterations: ', end.time-start.time)
```

```{r}
saveRDS(bn_uniform_result, file=rds_name('unif_ordering'))
object.size(bn_uniform_result[['chain']])
network_dag<-bn_uniform_result[['best']][['dag']]
chain_scores<-sapply(bn_uniform_result[['chain']], function(el)(el[['score']]))
chain_nodes_orderings<-lapply(bn_uniform_result[['chain']], function(el)(el[['nodes']]))
print(bn_uniform_result[['best']][['score']])
print_network(network_dag)
```

```{r}
print_scores_vs_ordering(chain_scores, chain_nodes_orderings)
```

```{r}
max_score_vs_iter(chain_scores)
```
```{r}
bic_score<-function(dag, data){
  return(score(dag , data = data , type = "bic"))
}
```

```{r}
dag_computation<-function(chain_scores,true_dag,chain_dags){  
  n_true_edges<-nrow(arcs(true_dag)) 
  # helper function to find the index of the cummax 
   
  find_index<-function(max_value,chain_scores){ 
      index<-which(chain_scores==max_value) 
      return(index) 
  } 
  
  #compare_dags helper function between the i-th network and true network 
  compare_dags <- function(index,true_dag, our_nets, n_true_edges=NULL) { 
      if(is.null(n_true_edges)){
        n_true_edges<-nrow(arcs(true_dag)) 
      }
      our_net<-our_nets[[index]] 
      our_dag<-create_bn_dag(our_net) 
      return(f1_error(our_dag, true_dag, n_true_edges=n_true_edges))
      #dag_shd <- shd(true_dag, our_dag) 
   
  } 
   
  indexes<-sapply(cummax(chain_scores),find_index,chain_scores) 
   
  #perform the comparison once the index is retrieved 
  errors<-sapply(indexes,function(index, true_dag, our_nets)(compare_dags(index, true_dag, our_nets, n_true_edges = n_true_edges)),true_dag,our_nets=chain_dags) 
  plot(1:length(errors),errors,xlab='Iteration',ylab='Error',main='Error Comparison',type='l',col='blue') 
  return(errors) 
 
} 
```

```{r}
dag_computation(
                sapply(bn_uniform_result[['chain']], function(el)(el[['score']])),
                true_dag.sachs,
                lapply(bn_uniform_result[["chain"]],function(el){el[["dag"]]})
                )
```
FILIPPO RUNNA QUESTA CELLA E COMMENTA (togli r da dalle graffe) LA SUCCESSIVA

No max parents, mcmc and uniform sampling of permuatations
```{}
unconstrained_bn_mcmc_results<-setNames(vector("list", 3), c("asia", "child", "sachs"))
UBMR<-unconstrained_bn_mcmc_results
for( dataset_name in c('asia','child','sachs')){
  n_iter<-1000
  start.time<-proc.time()
  UBMR[[dataset_name]]<-bn_mcmc(ztbinom_permutation_generator, k2_algorithm, datasets[[dataset_name]], n_iter)
  end.time<-proc.time()
  cat('bn_mcmc', dataset_name ,'duration',n_iter, 'iterations: ', end.time-start.time)
}
saveRDS(UBMR, file=rds_name('UBMR'))

unconstrained_bn_uniform_results<-setNames(vector("list", 3), c("asia", "child", "sachs"))
UBUR<-unconstrained_bn_uniform_results
for( dataset_name in c('asia','child','sachs')){
  n_iter<-1000
  start.time<-proc.time()
  UBUR[[dataset_name]]<-bn_uniform(k2_algorithm, datasets[[dataset_name]], n_iter)
  end.time<-proc.time()
  cat('bn_uniform', dataset_name ,'duration',n_iter, 'iterations: ', end.time-start.time)
}
saveRDS(UBUR, file=rds_name('UBUR'))
```


EMANUELE RUNNA QUESTA CELLA E COMMENTA (togli r da dalle graffe) LA PRECEDENTE

Set max parents, mcmc and uniform sampling of permuatations
```{r}
constrained_bn_mcmc_results<-setNames(vector("list", 3), c("asia", "child", "sachs"))
CBMR<-constrained_bn_mcmc_results
for( dataset_name in c('asia','child','sachs')){
  n_iter<-1000
  start.time<-proc.time()
  CBMR[[dataset_name]]<-bn_mcmc(ztbinom_permutation_generator, function(data)(k2_algorithm(data, max_parents = true_max_parents[[dataset_name]])), datasets[[dataset_name]], n_iter)
  end.time<-proc.time()
  cat('bn_mcmc', dataset_name ,'duration',n_iter, 'iterations: ', end.time-start.time)
}
saveRDS(CBMR, file=rds_name('CBMR'))

constrained_bn_uniform_results<-setNames(vector("list", 3), c("asia", "child", "sachs"))
CBUR<-constrained_bn_uniform_results
for( dataset_name in c('asia','child','sachs')){
  n_iter<-1000
  start.time<-proc.time()
  CBUR[[dataset_name]]<-bn_uniform(function(data)(k2_algorithm(data, max_parents = true_max_parents[[dataset_name]])), datasets[[dataset_name]], n_iter)
  end.time<-proc.time()
  cat('bn_uniform', dataset_name ,'duration',n_iter, 'iterations: ', end.time-start.time)
}
saveRDS(CBUR, file=rds_name('CBUR'))
```



